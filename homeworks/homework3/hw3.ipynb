{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VpbHkgMddS4"
   },
   "source": [
    "# Latent Variable Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00U99CI9X7GA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PksKzhF733Np"
   },
   "source": [
    "Below you will find the functions which visualizes 2d data and plots the obtained training curves.\n",
    "Study them carefully but do not change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqmHx9yW3SSH"
   },
   "outputs": [],
   "source": [
    "def visualize_2d_data(train_data, test_data, train_labels=None, test_labels=None):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax1.set_title('train', fontsize=16)\n",
    "    ax1.scatter(train_data[:, 0], train_data[:, 1], s=1, c=train_labels)\n",
    "    ax1.tick_params(labelsize=16)\n",
    "    # ax1.set_yticks(fontsize=16)\n",
    "    ax2.set_title('test', fontsize=16)\n",
    "    ax2.scatter(test_data[:, 0], test_data[:, 1], s=1, c=test_labels)\n",
    "    ax2.tick_params(labelsize=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_2d_samples(data, title):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.scatter(data[:, 0], data[:, 1], s=1)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_training_curves(train_losses, test_losses):\n",
    "    n_train = len(train_losses[list(train_losses.keys())[0]])\n",
    "    n_test = len(test_losses[list(train_losses.keys())[0]])\n",
    "    x_train = np.linspace(0, n_test - 1, n_train)\n",
    "    x_test = np.arange(n_test)\n",
    "\n",
    "    plt.figure()\n",
    "    for key, value in train_losses.items():\n",
    "        plt.plot(x_train, value, label=key + '_train')\n",
    "\n",
    "    for key, value in test_losses.items():\n",
    "        plt.plot(x_test, value, label=key + '_test')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AieobeyB372L"
   },
   "source": [
    "Here are the functions that we will you for training our model. Please, explore these functions carefully. You do not have to change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krGVH3SN3v_g"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, epoch, loss_key='total'):\n",
    "    model.train()\n",
    "    stats = defaultdict(list)\n",
    "    for x in train_loader:\n",
    "        x = x.cuda()\n",
    "        losses = model.loss(x)\n",
    "        optimizer.zero_grad()\n",
    "        losses[loss_key].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for k, v in losses.items():\n",
    "            stats[k].append(v.item())\n",
    "    return stats\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    stats = defaultdict(float)\n",
    "    with torch.no_grad():\n",
    "        for x in data_loader:\n",
    "            x = x.cuda()\n",
    "            losses = model.loss(x)\n",
    "            for k, v in losses.items():\n",
    "                stats[k] += v.item() * x.shape[0]\n",
    "\n",
    "        for k in stats.keys():\n",
    "            stats[k] /= len(data_loader.dataset)\n",
    "    return stats\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs, lr, loss_key='total'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = defaultdict(list)\n",
    "    test_losses = defaultdict(list)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, epoch, loss_key)\n",
    "        test_loss = eval_model(model, test_loader)\n",
    "\n",
    "        for k in train_loss.keys():\n",
    "            train_losses[k].extend(train_loss[k])\n",
    "            test_losses[k].append(test_loss[k])\n",
    "    return dict(train_losses), dict(test_losses)\n",
    "\n",
    "\n",
    "def get_latent_stats(model, test_data, batch_size=3000):\n",
    "    batch = next(iter(data.DataLoader(test_data, batch_size=batch_size, shuffle=True)))\n",
    "    batch = batch.cuda()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        mu_z, log_std_z = model(batch)[:2]\n",
    "        \n",
    "    mu_z = mu_z.cpu().numpy()\n",
    "    std_z = log_std_z.exp().cpu().numpy()\n",
    "\n",
    "    return mu_z, std_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZjBDpnudlJL"
   },
   "source": [
    "## Task 1 (4 points): VAE on 2d data\n",
    "\n",
    "In this task you will implement simple VAE model for 2d gaussian distribution.\n",
    "\n",
    "We will consider two cases: 2d univariate distribution and 2d multivariate distribution. The goal is to analyze the difference between these two cases and chosen VAE model.\n",
    "\n",
    "Below you will find data generation function. Look carefully, do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdTy9DLja_jV"
   },
   "outputs": [],
   "source": [
    "def generate_2d_data(count, mode='univariate'):\n",
    "    assert mode in ['univariate', 'multivariate']\n",
    "    np.random.seed(42)\n",
    "    mean = [[2.0, 3.0]]\n",
    "    sigma = [[3.0, 1.0]]\n",
    "    if mode == 'univariate':\n",
    "        rotate = [\n",
    "            [1.0, 0.0], \n",
    "            [0.0, 1.0]\n",
    "        ]\n",
    "    else:\n",
    "        rotate = [\n",
    "            [np.sqrt(2) / 2, np.sqrt(2) / 2], \n",
    "            [-np.sqrt(2) / 2, np.sqrt(2) / 2]\n",
    "        ]\n",
    "    data = mean + (np.random.randn(count, 2) * sigma).dot(rotate)\n",
    "    data = data.astype('float32')\n",
    "    split = int(0.7 * count)\n",
    "    train_data, test_data = data[:split], data[split:]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "JlZlxRm3a8Mi",
    "outputId": "fe90def9-3738-4e87-f71c-24badf2180b7"
   },
   "outputs": [],
   "source": [
    "COUNT = 15000\n",
    "\n",
    "train_data, test_data = generate_2d_data(COUNT, mode='multivariate')\n",
    "visualize_2d_data(train_data, test_data)\n",
    "\n",
    "train_data, test_data = generate_2d_data(COUNT, mode='univariate')\n",
    "visualize_2d_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vRYd_B3f3jz"
   },
   "source": [
    "The difference of these two cases in the form of covariance matrix.\n",
    "\n",
    "In multivariate case the matrix is non-diagonal, in univariate case it strictly diagonal. As you will see, our VAE model will work absolutely different for these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biYy9_rWd-DY"
   },
   "source": [
    "Now it is time to define our model. In this task you will create VAE model on 2d data. Model will be designed as:\n",
    "\n",
    "* the latent dimensionality is equal to 2, the same as the data dimensionality ($\\mathbf{z} \\in \\mathbb{R}^2$, $\\mathbf{x} \\in \\mathbb{R}^2$).\n",
    "* prior distribution $p(\\mathbf{z}) = \\mathcal{N}(0, I)$.\n",
    "* approximate variational distribution (or encoder) $q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x}))$. Here $\\boldsymbol{\\phi}$ denotes all parameters of the encoder neural network.\n",
    "* generative distribution (or decoder) $p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))$. Here $\\boldsymbol{\\theta}$ denotes all parameters of the decoder neural network.\n",
    "* We will consider only diagonal covariance matrices $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x})$, $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z})$.\n",
    "\n",
    "Model objective is ELBO:\n",
    "$$\n",
    "    \\mathcal{L}(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}) = \\mathbb{E}_{q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})} \\log p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) - KL (q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi}) || p(\\mathbf{z})).\n",
    "$$\n",
    "\n",
    "To make the expectation is independent of parameters $\\boldsymbol{\\phi}$, we will use reparametrization trick.\n",
    "\n",
    "To calculate the loss, you should derive\n",
    "- $\\log p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})$, note that generative distribution is $\\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))$.\n",
    "- KL between $\\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x}))$ and $\\mathcal{N}(0, I)$ (the similar, more general task was in your hw2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KN7NnFplkUSn"
   },
   "outputs": [],
   "source": [
    "class FullyConnectedMLP(nn.Module):\n",
    "    def __init__(self, input_shape, hiddens, output_shape):\n",
    "        assert isinstance(hiddens, list)\n",
    "        super().__init__()\n",
    "        self.input_shape = (input_shape,)\n",
    "        self.output_shape = (output_shape,)\n",
    "        self.hiddens = hiddens\n",
    "\n",
    "        model = []\n",
    "        # ====\n",
    "        # your code \n",
    "        # Stack some fully connected  (Linear) layers wirh relu activation.\n",
    "        # Note that you do not have to add relu after the last fc layer\n",
    "\n",
    "        \n",
    "        # ====\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply network that was defined in __init__ and return the output\n",
    "\n",
    "        \n",
    "        # ====\n",
    "\n",
    "\n",
    "class VAE2d(nn.Module):\n",
    "    def __init__(self, n_in, n_latent, enc_hidden_sizes, dec_hidden_sizes):\n",
    "        assert isinstance(enc_hidden_sizes, list)\n",
    "        assert isinstance(dec_hidden_sizes, list)\n",
    "        super().__init__()\n",
    "        self.n_latent = n_latent\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # define encoder and decoder networks\n",
    "        # the encoder takes n_in elements, has enc_hidden_sizes neurons in hidden layers \n",
    "        # and outputs 2 * n_latent (n_latent for means, and n_latent for std)\n",
    "        # the decoder takes n_latent elements, has dec_hidden_sizes neurons in hidden layers \n",
    "        # and outputs 2 * n_in (n_in for means, and n_in for std)\n",
    "        self.encoder = \n",
    "        self.decoder = \n",
    "        # ====\n",
    "\n",
    "    def prior(self, n):\n",
    "        # ====\n",
    "        # your code\n",
    "        # return n samples from prior distribution (we use standart normal for prior)\n",
    "\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        # now you have to return from the model \n",
    "        # - mu_z - means for variational distribution \n",
    "        # - mu_x - means for generative distribution\n",
    "        # - log_std_z - logarithm of std for variational distribution\n",
    "        # - log_std_x - logarithm of std for generative distribution\n",
    "        # we use logarithm, since the std is always positive\n",
    "        # to get std we will exponentiate it to get rid of this constraint\n",
    "        # 1) mu_z, log_std_z are outputs from the encoder\n",
    "        # 2) apply reparametrization trick to get z (input of decoder)\n",
    "        # (do not forget to use self.prior())\n",
    "        # 3) mu_x, log_std_x are outputs from the decoder\n",
    "\n",
    "        \n",
    "        # ====\n",
    "        return mu_z, log_std_z, mu_x, log_std_x\n",
    "\n",
    "    def loss(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply model to get mu_z, log_std_z, mu_x, log_std_x\n",
    "        # 2) compute reconstruction loss (it is the first term in ELBO)\n",
    "        # 3) compute KL loss (it is the second term in ELBO)\n",
    "        \n",
    "        \n",
    "        # ====\n",
    "        recon_loss = recon_loss.sum(1).mean()\n",
    "        kl_loss = kl_loss.sum(1).mean()\n",
    "\n",
    "        return {\n",
    "            'elbo_loss': recon_loss + kl_loss, \n",
    "            'recon_loss': recon_loss,\n",
    "            'kl_loss': kl_loss\n",
    "        }\n",
    "\n",
    "    def sample(self, n, noise=True):\n",
    "        with torch.no_grad():\n",
    "            # ====\n",
    "            # your code\n",
    "            # to sample from VAE model you have to sample from prior\n",
    "            # and then apply decoder to prior samples.\n",
    "            # parameter noise indicates whether to sample from decoder\n",
    "            # or just use means of generative distribution as samples\n",
    "            # 1) generate prior samples\n",
    "            # 2) apply decoder\n",
    "            # 3) sample from the decoder distribution if noise=True\n",
    "            \n",
    "            \n",
    "            if noise:\n",
    "                pass\n",
    "            else:\n",
    "                z = mu\n",
    "            # ====\n",
    "        return z.cpu().numpy()\n",
    "\n",
    "\n",
    "def solve_task(train_data, test_data, model, batch_size, epochs, lr):\n",
    "    # do not change this function\n",
    "    train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    train_losses, test_losses = train_model(\n",
    "        model, train_loader, test_loader, epochs=EPOCHS, lr=LR, loss_key='elbo_loss'\n",
    "    )\n",
    "    samples_noise = model.sample(3000, noise=True)\n",
    "    samples_nonoise = model.sample(3000, noise=False)\n",
    "\n",
    "    for key, value in test_losses.items():\n",
    "        print('{}: {:.4f}'.format(key, value[-1]))\n",
    "\n",
    "    plot_training_curves(train_losses, test_losses)\n",
    "    visualize_2d_samples(samples_noise, title='Samples with Decoder Noise')\n",
    "    visualize_2d_samples(samples_nonoise, title='Samples without Decoder Noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDAWwEs8eJWV"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters (2 hidden layers could be enough for encoder and decoder)\n",
    "ENC_HIDDEN_SIZES = \n",
    "DEC_HIDDEN_SIZES = \n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "# ====\n",
    "\n",
    "COUNT = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IdJaAD6Ls4hL",
    "outputId": "c7939751-fbe4-4b18-ab72-e867881793f2"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = generate_2d_data(COUNT, mode='multivariate')\n",
    "visualize_2d_data(train_data, test_data)\n",
    "\n",
    "model = VAE2d(2, 2, ENC_HIDDEN_SIZES, DEC_HIDDEN_SIZES).cuda()\n",
    "solve_task(train_data, test_data, model, BATCH_SIZE, EPOCHS, LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just look at these numbers and read comments after this task\n",
    "mu_z, std_z = get_latent_stats(model, test_data)\n",
    "\n",
    "print('mu_z = ', mu_z.mean(axis=0), '+-', mu_z.std(axis=0))\n",
    "print('std_z = ', std_z.mean(axis=0), '+-', std_z.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ob-yRJ8xe7Ns",
    "outputId": "eb171e95-42fd-4642-8560-df16689f3f65"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = generate_2d_data(COUNT, mode='univariate')\n",
    "visualize_2d_data(train_data, test_data)\n",
    "\n",
    "model = VAE2d(2, 2, ENC_HIDDEN_SIZES, DEC_HIDDEN_SIZES).cuda()\n",
    "solve_task(train_data, test_data, model, BATCH_SIZE, EPOCHS, LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just look at these numbers and read comments after this task\n",
    "mu_z, std_z = get_latent_stats(model, test_data)\n",
    "\n",
    "print('mu_z = ', mu_z.mean(axis=0), '+-', mu_z.std(axis=0))\n",
    "print('std_z = ', std_z.mean(axis=0), '+-', std_z.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRCmH4X_6tyQ"
   },
   "source": [
    "After training the model on these 2 datasets, you have to see on \"Samples without Decoder Noise\" figures. These figures show means of the geerative distribution. In the case of multivariate gaussian, the means are perfectly aligned with data distribution. \n",
    "Otherwise, in the univariate gaussian case you have to see strange figure. This happens due to so called \"posterior collapse\" (we will discuss it on the one of our lecture).\n",
    "\n",
    "Our posterior distribution $p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))$ is a univariate (covariance matrix is diagonal). Thus, to model univariate data distribution (second case) the model ignores latent code, cause we could model it without any information from latent space.\n",
    "\n",
    "If the decoder ignores latent code, the second term in ELBO (KL) could be low (variational disstribution, which is given by encoder model, is close to prior distribution for each datapoint). In the training curves you have to see that KL loss behaves differently in these two cases.\n",
    "\n",
    "The mean and std of variational distribution also proves this concept. For the second case you have to see that mean is almost zero and std is almost one.\n",
    "\n",
    "It is a real problem for generative models and we will discuss later how to overcome this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMACU__E8fmh"
   },
   "source": [
    "## Task 2 (4 points): VAE on CIFAR10 data\n",
    "\n",
    "In this task you will implement VAE model for real image distribution.\n",
    "\n",
    "Look carefully on the following helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-jbbpNLvfbW"
   },
   "outputs": [],
   "source": [
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    train_data, test_data = data['train'], data['test']\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def show_samples(samples, title, nrow=10):\n",
    "    samples = (torch.FloatTensor(samples) / 255).permute(0, 3, 1, 2)\n",
    "    grid_img = make_grid(samples, nrow=nrow)\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_data(data, title):\n",
    "    idxs = np.random.choice(len(data), replace=False, size=(100,))\n",
    "    images = train_data[idxs]\n",
    "    show_samples(images, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lm0ydjk3sXmR"
   },
   "source": [
    "Download data from here: https://drive.google.com/file/d/16j3nrJV821VOkkuRz7aYam8TyIXLnNme/view?usp=sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "lZPDypYLsN62",
    "outputId": "1fe6a620-75d3-4d8c-d04c-44eae0bf4c0e"
   },
   "outputs": [],
   "source": [
    "# change the path to the generated data\n",
    "train_data, test_data = load_pickle(os.path.join('drive', 'MyDrive', 'DGM', 'homework_supplementary', 'cifar10.pkl'))\n",
    "visualize_data(train_data, 'CIFAR10 samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9iFs78D8uoE"
   },
   "source": [
    "Here the model specification will be almost the same with the following differences:\n",
    "* Now our encoder and decoder will be convolutional.\n",
    "* We do not model covariance matrix in generative distribution (now it is identical). We will use means of the generative distribution as model samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYI_WLXnsM4g"
   },
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, input_shape, n_latent):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.n_latent = n_latent\n",
    "        # ====\n",
    "        # your code\n",
    "        # we suggest to use the following architecture\n",
    "        # conv2d(32) -> relu -> conv(64) -> relu -> conv(128) -> relu -> conv(256) -> fc(2 * n_latent)\n",
    "        # but we encourage you to create your own architecture\n",
    "        self.convs = nn.Sequential(\n",
    "            \n",
    "        )\n",
    "        self.fc = \n",
    "        # ====\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply convs\n",
    "        # 2) reshape the output to 2d matrix for last fc layer\n",
    "        # 3) apply fc layer\n",
    "        \n",
    "        \n",
    "        # ====\n",
    "        return mu, log_std\n",
    "        \n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, n_latent, output_shape):\n",
    "        super().__init__()\n",
    "        self.n_latent = n_latent\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        self.base_size = (128, output_shape[1] // 8, output_shape[2] // 8)\n",
    "        # ====\n",
    "        # your code\n",
    "        # we suggest to use the following architecture\n",
    "        # fc -> conv2dtranspose(128) -> relu -> conv2dtranspose(64) -> relu \n",
    "        # -> conv2dtranspose(32) -> relu -> conv2dtranspose(3)\n",
    "        # but we encourage you to create your own architecture\n",
    "        self.fc = \n",
    "        self.deconvs = nn.Sequential(\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply fc layer\n",
    "        # 2) reshape the output to 4d tensor \n",
    "        # 3) apply conv layers\n",
    "\n",
    "        \n",
    "        # ====\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, input_shape, n_latent):\n",
    "        super().__init__()\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.n_latent = n_latent\n",
    "        # ====\n",
    "        # your code\n",
    "        # define encoder with input size input_shape and output dim n_latent\n",
    "        # define decoder with input dim n_latent and output size input_shape\n",
    "        self.encoder = \n",
    "        self.decoder = \n",
    "        # ====\n",
    "\n",
    "    def prior(self, n):\n",
    "        # ====\n",
    "        # your code\n",
    "        # return n samples from prior distribution (we use standart normal for prior)\n",
    "        \n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply encoder to get mu_z, log_std_z\n",
    "        # 2) apply reparametrization trick (use self.prior)\n",
    "        # 3) apply decoder to get mu_x (which corresponds to reconstructed x)\n",
    "        \n",
    "        \n",
    "        # ====\n",
    "        return mu_z, log_std_z, x_recon\n",
    "        \n",
    "    def loss(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) make forward step to get mu_z, log_std_z, x_recon\n",
    "        # 2) calculate recon_loss\n",
    "        # 3) calcucalte kl_loss\n",
    "        \n",
    "        \n",
    "        # ==== \n",
    "        \n",
    "        recon_loss = recon_loss.sum(1).mean()\n",
    "        kl_loss = kl_loss.sum(1).mean()\n",
    "        return {\n",
    "            'elbo_loss': recon_loss + kl_loss, \n",
    "            'recon_loss': recon_loss,\n",
    "            'kl_loss': kl_loss\n",
    "        }\n",
    "\n",
    "    def sample(self, n):\n",
    "        with torch.no_grad():\n",
    "            # ====\n",
    "            # your code\n",
    "            # 1) generate prior samples\n",
    "            # 2) apply decoder\n",
    "            \n",
    "            \n",
    "            # ====\n",
    "            samples = torch.clamp(x_recon, -1, 1)\n",
    "        return samples.cpu().permute(0, 2, 3, 1).numpy() * 0.5 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "siM0O653zYQD",
    "outputId": "c1feed5c-712d-4707-eb0e-bf16f6d0a4cb"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "\n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "\n",
    "# ====\n",
    "\n",
    "# change the path to the data\n",
    "train_data, test_data = load_pickle(os.path.join('drive', 'MyDrive', 'DGM', 'homework_supplementary', 'cifar10.pkl'))\n",
    "\n",
    "# analyze the following code to understand what exactly happens here :)\n",
    "train_data = (np.transpose(train_data, (0, 3, 1, 2)) / 255. * 2 - 1).astype('float32')\n",
    "test_data = (np.transpose(test_data, (0, 3, 1, 2)) / 255. * 2 - 1).astype('float32')\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = ConvVAE((3, 32, 32), 16).cuda()\n",
    "train_losses, test_losses = train_model(\n",
    "    model, train_loader, test_loader, epochs=EPOCHS, lr=LR, loss_key='elbo_loss'\n",
    ")\n",
    "samples = model.sample(100) * 255.\n",
    "\n",
    "x = next(iter(test_loader))[:50].cuda()\n",
    "with torch.no_grad():\n",
    "    z, _ = model.encoder(x)\n",
    "    x_recon = torch.clamp(model.decoder(z), -1, 1)\n",
    "reconstructions = torch.stack((x, x_recon), dim=1).view(-1, 3, 32, 32) * 0.5 + 0.5\n",
    "reconstructions = reconstructions.permute(0, 2, 3, 1).cpu().numpy() * 255\n",
    "\n",
    "x = next(iter(test_loader))[:20].cuda()\n",
    "with torch.no_grad():\n",
    "    z, _ = model.encoder(x)\n",
    "    z1, z2 = z.chunk(2, dim=0)\n",
    "    interps = [model.decoder(z1 * (1 - alpha) + z2 * alpha) for alpha in np.linspace(0, 1, 10)]\n",
    "    interps = torch.stack(interps, dim=1).view(-1, 3, 32, 32)\n",
    "    interps = torch.clamp(interps, -1, 1) * 0.5 + 0.5\n",
    "interps = interps.permute(0, 2, 3, 1).cpu().numpy() * 255\n",
    "\n",
    "samples, reconstructions, interps = samples.astype('float32'), reconstructions.astype('float32'), interps.astype('float32')\n",
    "for key, value in test_losses.items():\n",
    "    print('{}: {:.4f}'.format(key, value[-1]))\n",
    "plot_training_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIJoj_OY9LAU"
   },
   "source": [
    "## Task 3 (5 points): RealNVP on 2d data\n",
    "\n",
    "In this task you will implement RealNVP model on 2d moons dataset. \n",
    "\n",
    "The following function generates the data (do not change it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wDA8Zo5SYbZ"
   },
   "outputs": [],
   "source": [
    "def generate_moons_data(count):\n",
    "    data, labels = make_moons(n_samples=count, noise=0.1)\n",
    "    data = data.astype('float32')\n",
    "    split = int(0.8 * count)\n",
    "    train_data, test_data = data[:split], data[split:]\n",
    "    train_labels, test_labels = labels[:split], labels[split:]\n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "Mh5vwoc-bgfE",
    "outputId": "e15dad7d-b66b-47a9-ab27-56e135861f39"
   },
   "outputs": [],
   "source": [
    "COUNT = 5000\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = generate_moons_data(COUNT)\n",
    "visualize_2d_data(train_data, test_data, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8C4o9wx9x2D"
   },
   "source": [
    "See original paper for model description (https://arxiv.org/abs/1605.08803).\n",
    "\n",
    "The model is a sequence of affine coupling layers. Note that you have to permute the features that will left unchanged between different layers (change order of $\\\\mathbf{x}_1$ and $\\mathbf{2}$ in formulas below).\n",
    "\n",
    "Forward transform:\n",
    "$$\n",
    "    \\begin{cases}\n",
    "        \\mathbf{y}_1 &= \\mathbf{x}_1; \\\\\n",
    "        \\mathbf{y}_2 &= \\mathbf{x}_2 \\odot \\exp (s(\\mathbf{x}_1)) + t(\\mathbf{x}_1).\n",
    "    \\end{cases} \n",
    "$$\n",
    "\n",
    "Inverse transform:\n",
    "$$\n",
    "    \\begin{cases}\n",
    "        \\mathbf{x}_1 &= \\mathbf{y}_1; \\\\\n",
    "        \\mathbf{x}_2 &= (\\mathbf{y}_2 - t(\\mathbf{y}_1)) \\odot \\exp ( - s(\\mathbf{y}_1)).\n",
    "    \\end{cases} \n",
    "$$\n",
    "\n",
    "Here $s(\\cdot)$ and $t(\\cdot)$ are outputs of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11DTN8IKB4DC"
   },
   "outputs": [],
   "source": [
    "class AffineTransform(nn.Module):\n",
    "    def __init__(self, partition_type, n_hiddens):\n",
    "        assert isinstance(n_hiddens, list)\n",
    "        super().__init__()\n",
    "        self.mask = self.build_mask(partition_type=partition_type)\n",
    "        self.scale = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "        self.scale_shift = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "        self.mlp = FullyConnectedMLP(input_shape=2, hiddens=n_hiddens, output_shape=2)\n",
    "\n",
    "    def build_mask(self, partition_type):\n",
    "        assert partition_type in {\"left\", \"right\"}\n",
    "        # ====\n",
    "        # your code\n",
    "        # mask is extremely simple\n",
    "        # it is a float tensor of two scalars (1.0 and 0.0)\n",
    "        # the partition_type defines the order of these two scalars\n",
    "        if partition_type == \"left\":\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # ====\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x, invert=False):\n",
    "        # ====\n",
    "        # your code\n",
    "        # you have to mask our input x\n",
    "        # repeat mask batch_size times and apply x on mask\n",
    "        \n",
    "        x_masked = \n",
    "        # ====\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply mlp to masked input to get log_s and t \n",
    "        log_s, t = \n",
    "        # ====\n",
    "\n",
    "        # this formula is described in Section 4.1 in original paper\n",
    "        # just left it unchanged\n",
    "        log_s = self.scale * torch.tanh(log_s) + self.scale_shift\n",
    "\n",
    "        # note that we invert mask here\n",
    "        t = t * (1.0 - mask)\n",
    "        log_s = log_s * (1.0 - mask)\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # if invert=True use reverse transform, else use forward transform\n",
    "        if invert:  # inverting the transformation\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "        # ====\n",
    "\n",
    "        # the output is transformed input \n",
    "        # and logarithm of jacobian which equals to log_s\n",
    "        return x, log_s\n",
    "\n",
    "\n",
    "class RealNVP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # base distribution is normal\n",
    "        self.prior = torch.distributions.Normal(torch.tensor(0.), torch.tensor(1.))\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply sequence of AffineTransform with alternating partition_type\n",
    "        # 6 layers is sufficient (with 2 hidden layers in each affine layer)\n",
    "        self.transforms = nn.ModuleList([\n",
    "            \n",
    "        ])\n",
    "        # ====\n",
    "        \n",
    "    def forward(self, x, invert=False):\n",
    "        z = x\n",
    "        log_det = 0.0\n",
    "        transforms = self.transforms[::-1] if invert else self.transforms\n",
    "\n",
    "        for transform in transforms:\n",
    "            z, delta_log_det = transform(z, invert)\n",
    "            log_det += delta_log_det\n",
    "\n",
    "        return z, log_det\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) make forward pass with right inverse flag\n",
    "        # 2) sum log_det with log of base distribution\n",
    "        \n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def loss(self, x):\n",
    "        return {'nll_loss': -self.log_prob(x).mean()}\n",
    "\n",
    "    def sample(self, n):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) sample from the prior\n",
    "        # 2) apply the forward pass with the right inverse flag\n",
    "        # 3) return only the first output of forward pass\n",
    "        \n",
    "        \n",
    "        # ===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4x06Stg548_R"
   },
   "source": [
    "Here the helper functions for visualization. Look at them carefully and do not change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IobSqemwTgyB"
   },
   "outputs": [],
   "source": [
    "def show_2d_latents(latents, labels=None, title='Latent Space'):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if labels is None:\n",
    "        labels = 'green'\n",
    "    plt.scatter(latents[:, 0], latents[:, 1], s=1, c=labels)\n",
    "    plt.xlabel('z1')\n",
    "    plt.ylabel('z2')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def show_2d_densities(densities, title='Densities'):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    dx, dy = 0.025, 0.025\n",
    "    x_lim = (-1.5, 2.5)\n",
    "    y_lim = (-1, 1.5)\n",
    "    y, x = np.mgrid[slice(y_lim[0], y_lim[1] + dy, dy),\n",
    "                    slice(x_lim[0], x_lim[1] + dx, dx)]\n",
    "    plt.pcolor(x, y, densities.reshape([y.shape[0], y.shape[1]]))\n",
    "    plt.pcolor(x, y, densities.reshape([y.shape[0], y.shape[1]]))\n",
    "    plt.xlabel('z1')\n",
    "    plt.ylabel('z2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 835
    },
    "id": "SNR4FHBATQMO",
    "outputId": "bc136595-c30a-4006-a8b8-0b2244736fb3"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "\n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "\n",
    "# ====\n",
    "\n",
    "COUNT = 5000\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = generate_moons_data(COUNT)\n",
    "\n",
    "loader_args = dict(batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = data.DataLoader(train_data, **loader_args)\n",
    "test_loader = data.DataLoader(test_data, **loader_args)\n",
    "\n",
    "# model\n",
    "real_nvp = RealNVP().cuda()\n",
    "\n",
    "# train\n",
    "train_losses, test_losses = train_model(\n",
    "    real_nvp, train_loader, test_loader, epochs=EPOCHS, lr=LR, loss_key='nll_loss'\n",
    ")\n",
    "\n",
    "# heatmap\n",
    "dx, dy = 0.025, 0.025\n",
    "x_lim = (-1.5, 2.5)\n",
    "y_lim = (-1, 1.5)\n",
    "y, x = np.mgrid[slice(y_lim[0], y_lim[1] + dy, dy),\n",
    "                slice(x_lim[0], x_lim[1] + dx, dx)]\n",
    "mesh_xs = torch.FloatTensor(np.stack([x, y], axis=2).reshape(-1, 2)).cuda()\n",
    "densities = np.exp(real_nvp.log_prob(mesh_xs).cpu().detach().numpy())\n",
    "\n",
    "# latents\n",
    "z = real_nvp(torch.FloatTensor(train_data).cuda())[0]\n",
    "latents = z.cpu().detach().numpy()\n",
    "\n",
    "plot_training_curves(train_losses, test_losses)\n",
    "show_2d_densities(densities)\n",
    "show_2d_latents(latents, train_labels)\n",
    "\n",
    "x_samples = real_nvp.sample(4000).cpu().detach().numpy()\n",
    "show_2d_latents(x_samples)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
