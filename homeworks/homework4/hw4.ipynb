{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieenT6NCp_OK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision.utils import make_grid\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFQTGwLhUOsa"
   },
   "source": [
    "These functions are helpers that will train your models and visualize the results. You do not have to change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "XAPPZBoao-fZ",
    "outputId": "119604d8-33df-4edf-94a6-7ec829c09e19"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGeKsFUqqNtZ"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, epoch, loss_key='total'):\n",
    "    model.train()\n",
    "    stats = defaultdict(list)\n",
    "    for x in train_loader:\n",
    "        x = x.cuda()\n",
    "        losses = model.loss(x)\n",
    "        optimizer.zero_grad()\n",
    "        losses[loss_key].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for k, v in losses.items():\n",
    "            stats[k].append(v.item())\n",
    "    return stats\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    stats = defaultdict(float)\n",
    "    with torch.no_grad():\n",
    "        for x in data_loader:\n",
    "            x = x.cuda()\n",
    "            losses = model.loss(x)\n",
    "            for k, v in losses.items():\n",
    "                stats[k] += v.item() * x.shape[0]\n",
    "\n",
    "        for k in stats.keys():\n",
    "            stats[k] /= len(data_loader.dataset)\n",
    "    return stats\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs, lr, loss_key='total'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = defaultdict(list)\n",
    "    test_losses = defaultdict(list)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, epoch, loss_key)\n",
    "        test_loss = eval_model(model, test_loader)\n",
    "\n",
    "        for k in train_loss.keys():\n",
    "            train_losses[k].extend(train_loss[k])\n",
    "            test_losses[k].append(test_loss[k])\n",
    "    return dict(train_losses), dict(test_losses)\n",
    "\n",
    "\n",
    "def show_2d_latents(latents, labels=None, title='Latent Space'):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if labels is None:\n",
    "        labels = 'green'\n",
    "    plt.scatter(latents[:, 0], latents[:, 1], s=1, c=labels)\n",
    "    plt.xlabel('z1')\n",
    "    plt.ylabel('z2')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def show_2d_densities(densities, title='Densities'):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    dx, dy = 0.025, 0.025\n",
    "    x_lim = (-1.5, 2.5)\n",
    "    y_lim = (-1, 1.5)\n",
    "    y, x = np.mgrid[slice(y_lim[0], y_lim[1] + dy, dy),\n",
    "                    slice(x_lim[0], x_lim[1] + dx, dx)]\n",
    "    plt.pcolor(x, y, densities.reshape([y.shape[0], y.shape[1]]))\n",
    "    plt.pcolor(x, y, densities.reshape([y.shape[0], y.shape[1]]))\n",
    "    plt.xlabel('z1')\n",
    "    plt.ylabel('z2')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_training_curves(train_losses, test_losses):\n",
    "    n_train = len(train_losses[list(train_losses.keys())[0]])\n",
    "    n_test = len(test_losses[list(train_losses.keys())[0]])\n",
    "    x_train = np.linspace(0, n_test - 1, n_train)\n",
    "    x_test = np.arange(n_test)\n",
    "\n",
    "    plt.figure()\n",
    "    for key, value in train_losses.items():\n",
    "        plt.plot(x_train, value, label=key + '_train')\n",
    "\n",
    "    for key, value in test_losses.items():\n",
    "        plt.plot(x_test, value, label=key + '_test')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    train_data, test_data = data['train'], data['test']\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def show_samples(samples, title, nrow=10):\n",
    "    samples = (torch.FloatTensor(samples) / 255).permute(0, 3, 1, 2)\n",
    "    grid_img = make_grid(samples, nrow=nrow)\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_data(data, title):\n",
    "    idxs = np.random.choice(len(data), replace=False, size=(100,))\n",
    "    images = train_data[idxs]\n",
    "    show_samples(images, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozYMt9IWkHun"
   },
   "source": [
    "# Task 1: Autoregressive flows on 2d data\n",
    "\n",
    "In this task you will train autoregressive flow mod on 2d data. Let generate and visualize train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7UvRoX-rQL1"
   },
   "outputs": [],
   "source": [
    "def generate_moons_data(count):\n",
    "    data, labels = make_moons(n_samples=count, noise=0.1)\n",
    "    data = data.astype('float32')\n",
    "    split = int(0.8 * count)\n",
    "    train_data, test_data = data[:split], data[split:]\n",
    "    train_labels, test_labels = labels[:split], labels[split:]\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "\n",
    "def visualize_2d_data(train_data, test_data, train_labels=None, test_labels=None):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax1.set_title('train', fontsize=16)\n",
    "    ax1.scatter(train_data[:, 0], train_data[:, 1], s=1, c=train_labels)\n",
    "    ax1.tick_params(labelsize=16)\n",
    "    ax2.set_title('test', fontsize=16)\n",
    "    ax2.scatter(test_data[:, 0], test_data[:, 1], s=1, c=test_labels)\n",
    "    ax2.tick_params(labelsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "-ChZ2JD0rSo2",
    "outputId": "8b0a727f-b228-4c91-9d2d-9db064f913be"
   },
   "outputs": [],
   "source": [
    "COUNT = 5000\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = generate_moons_data(COUNT)\n",
    "visualize_2d_data(train_data, test_data, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-EzAbkBVFzN"
   },
   "source": [
    "Let define our model. We will use autoregressive flow model. \n",
    "\n",
    "You have to maximize log-likelihood (or equivalently minimize negative log-likelihood):\n",
    "$$\n",
    "    \\log p(\\mathbf{x} | \\boldsymbol{\\theta}) = \\log p(\\mathbf{z}) + \\log \\det \\left| \\frac{d \\mathbf{z}}{d \\mathbf{x}} \\right|.\n",
    "$$\n",
    "\n",
    "Note: this is equivalent to the minimization of forward KL.\n",
    "\n",
    "The base distribution will be 2d Uniform: $\\mathbf{z} \\sim p(\\mathbf{z}) = U[0, 1]$ (here $\\mathbf{z} = (z_1, z_2)$).\n",
    "\n",
    "Since we will you autoregressive model, the Jacobian will be triangular.\n",
    "\n",
    "As we discussed in the lecture cumulative distribution function (CDF) of random variable is distributed uniformly. Thus, we will use CDF to map our data to the latent space. We assume that our data comes from mixture of gaussians distribution, which is defined by:\n",
    "* $\\mathbf{w}$ - weights of mixture components,\n",
    "* $\\boldsymbol{\\mu}$ - locations (means) of each gaussian, \n",
    "* $\\boldsymbol{\\sigma}$ - standart deviations of each gaussian.\n",
    "\n",
    "The mapping function is the following:\n",
    "$$\n",
    "    z_i = F(x_i, \\mathbf{w}(x_{1:i-1}), \\boldsymbol{\\mu}(x_{1:i-1}), \\boldsymbol{\\sigma}(x_{1:i-1})).\n",
    "$$\n",
    "Here function $F$ is just CDF of gaussian mixture:\n",
    "$$\n",
    "    F(x, \\mathbf{w}, \\boldsymbol{\\mu}, \\boldsymbol{\\sigma}) = \\int_{-\\infty}^x f(t) dt, \\quad \\text{where} \\quad f(t) = \\sum_{k=1}^K w_k \\mathcal{N}(t | \\mu_k, \\sigma^2_k)\n",
    "$$\n",
    "\n",
    "For 2d case you will get\n",
    "\n",
    "\\begin{align*}\n",
    "    z_1 &= F(x_1, \\mathbf{w}, \\boldsymbol{\\mu}, \\boldsymbol{\\sigma}) \\\\\n",
    "    z_2 &= F(x_1, \\mathbf{w}(x_1), \\boldsymbol{\\mu}(x_1), \\boldsymbol{\\sigma}(x_1)).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_vaAsPSqsYd"
   },
   "outputs": [],
   "source": [
    "class MixtureCDFFlow(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_components=4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # this is a base distribution\n",
    "        self.base_dist = Uniform(0.0, 1.0)\n",
    "        # this is a distribution of one of the mixture component\n",
    "        self.mixture_dist = Normal\n",
    "        self.n_components = n_components\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # define mixture parameters (location, log_scale, weights_logits)\n",
    "        # location - is a means of gaussian mixture components\n",
    "        # log_scale - is a logarithm of standard deviation for gaussian mixture component\n",
    "        # (since std should be positive we use logarithm and then exponentiate it)\n",
    "        # weights_logits - logits (before softmax) of gaussian mixture weights\n",
    "        # these parameters are trainable\n",
    "        self.loc = \n",
    "        self.log_scale = \n",
    "        self.weight_logits = \n",
    "        # ====\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 1\n",
    "        # ====\n",
    "        # your code\n",
    "        # to get weights of each component apply softmax to weight_logits\n",
    "        weights = \n",
    "        # ====\n",
    "\n",
    "        # ====\n",
    "        # your code \n",
    "        # 1) find CDF value for each component \n",
    "        # use .cdf() method of self.mixture_dist with self.loc and self.log_scale\n",
    "        # (do not forget that scale is logarithmic, we need to exponentiate)\n",
    "        # 2) multiply the cdf for each gaussian to the mixture weights \n",
    "        # 3) sum these values across components\n",
    "        z = \n",
    "        # ====\n",
    "\n",
    "        # ====\n",
    "        # your code \n",
    "        # calculate logarithm of determinant\n",
    "        # 1) use .log_prob() method of self.mixture_dist\n",
    "        # 2) exponentiate the results\n",
    "        # 3) multiply by weights\n",
    "        # 4) take the logarithm\n",
    "        log_det = \n",
    "        # ====\n",
    "\n",
    "        return z, log_det\n",
    "\n",
    "\n",
    "class FullyConnectedMLP(nn.Module):\n",
    "    def __init__(self, input_shape, hiddens, output_shape):\n",
    "        assert isinstance(hiddens, list)\n",
    "        super().__init__()\n",
    "        self.input_shape = (input_shape,)\n",
    "        self.output_shape = (output_shape,)\n",
    "        self.hiddens = hiddens\n",
    "\n",
    "        model = []\n",
    "        prev_h = input_shape\n",
    "        for h in hiddens:\n",
    "            # ====\n",
    "            # your code\n",
    "            # just add linear layer followed by relu\n",
    "            # ====\n",
    "            prev_h = h\n",
    "        model.append(nn.Linear(hiddens[-1], output_shape))\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        return self.net(x).view(batch_size, *self.output_shape)\n",
    "\n",
    "\n",
    "class AutoregressiveFlow(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        mix_comp_z1=5,  \n",
    "        mix_comp_z2=5,\n",
    "        mlp_hiddens=[64, 64, 64]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # this is a base distribution\n",
    "        self.base_dist = Uniform(torch.tensor(0.0).cuda(), torch.tensor(1.0).cuda())\n",
    "        # this is a distribution of one of the mixture component\n",
    "        self.mixture_dist = Normal\n",
    "        self.mix_comp_z2 = mix_comp_z2\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # define mixture cdf flow defined above for z_1\n",
    "        self.dim1_flow = \n",
    "        # ====\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # define mlp with mlp_hiddens hidden units\n",
    "        # mlp will output 3 sets ot parameters (weights_logits, loc, log_scale)\n",
    "        self.mlp = \n",
    "        # =====\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2 = torch.chunk(x, 2, dim=1)\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply mixture cdf flow to the x1\n",
    "        z1, log_det1 = \n",
    "        # ====\n",
    "        \n",
    "        # ====\n",
    "        # your code\n",
    "        # apply mlp to x1, you will get parameters of second mixture components\n",
    "        loc, log_scale, weight_logits = \n",
    "        # ====\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply softmax to weight_logits to get mixture weights\n",
    "        weights = \n",
    "        # ====\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) find CDF value for z_2 \n",
    "        # (use .cdf() method of self.mixture_dist with self.loc and self.log_scale)\n",
    "        # 2) multiply the cdf for each gaussian to the mixture weights \n",
    "        # 3) sum these values across components\n",
    "        z2 = \n",
    "        # ====\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) use .log_prob() method of self.mixture_dist\n",
    "        # 2) exponentiate\n",
    "        # 3) multiply by weights \n",
    "        # 4) take logarithm\n",
    "        log_det2 = \n",
    "        # ====\n",
    "\n",
    "        # concatenate the results and return\n",
    "        z = torch.cat([z1.unsqueeze(1), z2.unsqueeze(1)], dim=1)\n",
    "        log_det = torch.cat([log_det1.unsqueeze(1), log_det2.unsqueeze(1)], dim=1)\n",
    "        return z, log_det\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply the model to get z and log_det\n",
    "        z, log_det = \n",
    "        # ====\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # return the log-likelihood\n",
    "        # you have to sum log of base distr + log of determinant\n",
    "        # but think about base distribution (it is uniform), what do we get for log of uniform density?\n",
    "        return \n",
    "        # ====\n",
    "\n",
    "    def loss(self, x):\n",
    "        # loss is just negative value of log prob\n",
    "        return {'total': -self.log_prob(x).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTbbUtvNqzvl"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "# ====\n",
    "MIX_COMP_Z1 = 3\n",
    "MIX_COMP_Z2 = 3\n",
    "MLP_HIDDENS = [64, 64, 64]\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "ar_flow = AutoregressiveFlow(\n",
    "    mix_comp_z1=MIX_COMP_Z1,\n",
    "    mix_comp_z2=MIX_COMP_Z2,\n",
    "    mlp_hiddens=MLP_HIDDENS\n",
    ").cuda()\n",
    "\n",
    "train_losses, test_losses = train_model(ar_flow, train_loader, test_loader, epochs=EPOCHS, lr=LR)\n",
    "assert test_losses['total'][-1] < 0.58\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 835
    },
    "id": "pHE8AnLcuvuU",
    "outputId": "d62aa0f9-5366-4694-b838-59b27a0e2cbd"
   },
   "outputs": [],
   "source": [
    "dx, dy = 0.025, 0.025\n",
    "x_lim = (-1.5, 2.5)\n",
    "y_lim = (-1, 1.5)\n",
    "y, x = np.mgrid[slice(y_lim[0], y_lim[1] + dy, dy),\n",
    "                slice(x_lim[0], x_lim[1] + dx, dx)]\n",
    "mesh_xs = torch.FloatTensor(np.stack([x, y], axis=2).reshape(-1, 2)).cuda()\n",
    "densities = np.exp(ar_flow.log_prob(mesh_xs).cpu().detach().numpy())\n",
    "\n",
    "z, _ = ar_flow(torch.FloatTensor(train_data).cuda())\n",
    "latents = z.cpu().detach().numpy()\n",
    "\n",
    "plot_training_curves(train_losses, test_losses)\n",
    "show_2d_densities(densities)\n",
    "show_2d_latents(latents, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJl2n51n1I45"
   },
   "source": [
    "Note that it is not trivial to invert this flow. Hence, we can not easily sample from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HLQR8FikQd_"
   },
   "source": [
    "# Task 2: VAE with Autoregressive prior on CIFAR10\n",
    "\n",
    "In this task you will fit Variational Lossy Antoencoder (https://arxiv.org/abs/1611.02731) model to the CIFAR10 dataset (https://drive.google.com/file/d/16j3nrJV821VOkkuRz7aYam8TyIXLnNme/view?usp=sharing).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "dTW5lhwv-J9w",
    "outputId": "547ef767-9fb7-4cee-c065-8a2e7eaf33cb"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = load_pickle(os.path.join('drive', 'MyDrive', 'DGM', 'homework_supplementary', 'cifar10.pkl'))\n",
    "visualize_data(train_data, 'CIFAR10 samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMNE7poKkqio"
   },
   "source": [
    "The model consists of:\n",
    "* convolutional encoder (variational posterior destrituion $q(\\mathbf{z} | \\mathbf{x})$)\n",
    "* convolutional decoder $p(\\mathbf{x} | \\mathbf{z})$\n",
    "* **autoregressive prior** $p(\\mathbf{z})$\n",
    "\n",
    "We will use MADE model for autoregressive prior. MADE Autoregressive flow is a mapping from $\\mathbf{z}$ to $\\boldsymbol{\\varepsilon}$). We denote it $\\mathbf{z} = \\mathbf{g}(\\boldsymbol{\\varepsilon}, \\boldsymbol{\\lambda}) = \\mathbf{f}(\\boldsymbol{\\varepsilon}, \\boldsymbol{\\lambda})^{-1}$ \n",
    "The mapping from $z$ to $\\boldsymbol{\\varepsilon}$ has the form:\n",
    "$$\n",
    "    \\varepsilon_i = z_i * \\sigma(\\mathbf{z}_{1:i-1}) + \\mu(\\mathbf{z}_{1:i-1})\n",
    "$$\n",
    "\n",
    "Note that it is parallel. We use this during training.\n",
    "\n",
    "However, the reverse mapping will be parallel:\n",
    "\n",
    "$$\n",
    "    z_i = \\frac{\\varepsilon_i - \\mu(\\mathbf{z}_{1:i-1})}{\\sigma(\\mathbf{z}_{1:i-1})}\n",
    "$$\n",
    "\n",
    "We will use it during sampling from the model. Despite the fact that this transform is sequential, it is performed in the latent space, so it is quite efficient.\n",
    "\n",
    "The ELBO objective in this task is:\n",
    "$$\n",
    "-E_{q(\\mathbf{z}|\\mathbf{x})}[\\log{p(\\mathbf{x}|\\mathbf{z})}] + E_{q(\\mathbf{z}|\\mathbf{x})}[\\log{q(\\mathbf{z}|\\mathbf{x})} - \\log{p(\\mathbf{z})}]\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\log{p(\\mathbf{z})} = \\log{p(\\boldsymbol{\\varepsilon})} + \\log{\\det\\left|\\frac{d\\boldsymbol{\\varepsilon}}{d\\mathbf{z}}\\right|}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3AxLZam9cb-"
   },
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, input_shape, n_latent):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.n_latent = n_latent\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # we suggest to use the following architecture\n",
    "        # conv2d(32) -> relu -> conv(64) -> relu -> conv(128) -> relu -> conv(256) -> fc(2 * n_latent)\n",
    "        # but we encourage you to create your own architecture\n",
    "        self.convs = nn.Sequential(\n",
    "            \n",
    "        )\n",
    "        conv_out_dim = input_shape[1] // 8 * input_shape[2] // 8 * 256\n",
    "        self.fc = nn.Linear(conv_out_dim, 2 * n_latent)\n",
    "        # ====\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply convs\n",
    "        # 2) reshape the output to 2d matrix for last fc layer\n",
    "        # 3) apply fc layer\n",
    "        # ====\n",
    "        return mu, log_std\n",
    "        \n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, n_latent, output_shape):\n",
    "        super().__init__()\n",
    "        self.n_latent = n_latent\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        self.base_size = (128, output_shape[1] // 8, output_shape[2] // 8)\n",
    "        # ====\n",
    "        # your code\n",
    "        # we suggest to use the following architecture\n",
    "        # fc -> conv2dtranspose(128) -> relu -> conv2dtranspose(64) -> relu \n",
    "        # -> conv2dtranspose(32) -> relu -> conv2dtranspose(3)\n",
    "        # but we encourage you to create your own architecture\n",
    "        self.fc = nn.Linear(n_latent, np.prod(self.base_size))\n",
    "        self.deconvs = nn.Sequential(\n",
    "            \n",
    "        )\n",
    "        # ====\n",
    "\n",
    "    def forward(self, z):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply fc layer\n",
    "        # 2) reshape the output to 4d tensor \n",
    "        # 3) apply conv layers\n",
    "        # ====\n",
    "        return out\n",
    "\n",
    "\n",
    "class MaskedLinear(nn.Linear):\n",
    "    # do not change this class\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.register_buffer('mask', torch.ones(out_features, in_features))\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.mask.data.copy_(torch.from_numpy(mask.astype(np.uint8).T))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.mask * self.weight, self.bias)\n",
    "\n",
    "\n",
    "class MADE(nn.Module):\n",
    "    def __init__(self, nin, bins, hidden_sizes):\n",
    "        super().__init__()\n",
    "        self.nin = nin\n",
    "        self.nout = nin * bins\n",
    "        self.bins = bins\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        # we will use the trivial ordering of input units\n",
    "        self.ordering = np.arange(self.nin)\n",
    "\n",
    "        self.net = []\n",
    "        hs = [self.nin] + self.hidden_sizes + [self.nout]\n",
    "        # ====\n",
    "        # your code\n",
    "        # define a simple MLP neural net\n",
    "        # stack MaskedLinear layers followed by ReLU\n",
    "        # (do not add ReLU after the last MaskedLinear)\n",
    "        # ====\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "        self.create_mask() # builds the initial self.m connectivity\n",
    "\n",
    "    def create_mask(self):\n",
    "        self.m = {}\n",
    "        L = len(self.hidden_sizes)\n",
    "\n",
    "        # the initial ordering is trivial\n",
    "        self.m[-1] = self.ordering\n",
    "        # ====\n",
    "        # your code\n",
    "        # for each layer and for each hidden unit we have to assign the random number from 1 to self.nin - 1\n",
    "        # note that it is more efficient to assign random number from self.m[l - 1].min() to self.nin - 1\n",
    "        for l in range(L):\n",
    "            self.m[l] = \n",
    "        # ====\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) for each hidden layer connect each hidden unit with random number k \n",
    "        #    with the previous layer units which has the number is less or equal than k.\n",
    "        # 2) for the last mask: connect each output unit with number k with the previous layer units \n",
    "        #    which has the number is less than k.\n",
    "        masks = [?? for l in range(L)]\n",
    "        masks.append(??)\n",
    "        # ====\n",
    "\n",
    "        masks[-1] = np.repeat(masks[-1], self.bins, axis=1)\n",
    "        self.masks = masks\n",
    "\n",
    "        # set the masks in all MaskedLinear layers\n",
    "        layers = [l for l in self.net.modules() if isinstance(l, MaskedLinear)]\n",
    "        for l, m in zip(layers, masks):\n",
    "            l.set_mask(m)\n",
    "\n",
    "    def visualize_masks(self):\n",
    "        for m in self.masks:\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            plt.imshow(m, cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        out = x.view(batch_size, self.nin)\n",
    "        out = self.net(out)\n",
    "        out = out.view(batch_size, self.nin, self.bins)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AFVAE(nn.Module):\n",
    "    def __init__(self, input_shape, latent_size, use_afp=False):\n",
    "        super().__init__()\n",
    "        assert len(input_shape) == 3\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_size = latent_size\n",
    "        # if the flag is False, we will get standard VAE model without autoregressive prior\n",
    "        self.use_afp = use_afp\n",
    "\n",
    "        if use_afp:\n",
    "            # made model has latent_size nin, 2 bins (for mean and std of output normal) \n",
    "            self.made = MADE(latent_size, 2, hidden_sizes=[512, 512])\n",
    "        self.encoder = ConvEncoder(input_shape, latent_size)\n",
    "        self.decoder = ConvDecoder(latent_size, input_shape)\n",
    "\n",
    "    def loss(self, x):\n",
    "        # we normalize input to [-1; 1] range\n",
    "        x = 2 * x.float() - 1\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply encoder to x to get variational distribution parameters\n",
    "        mu_z, log_std_z = \n",
    "        # ====\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # sample z from variational distribution (reparametrization trick)\n",
    "        z = \n",
    "        # ====\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply decoder to get reconstructed x\n",
    "        x_recon = \n",
    "        # ====\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # calculate recon_loss\n",
    "        recon_loss = \n",
    "        # ====\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # enc log prob is logarithm of normal distribution density on z: log q(z|x)\n",
    "        enc_log_prob = \n",
    "        # ====\n",
    "\n",
    "        if self.use_afp:\n",
    "            # ====\n",
    "            # your code\n",
    "            # apply MADE model to z\n",
    "            out = \n",
    "            # ====\n",
    "            mu, log_std = out.chunk(2, dim=-1)\n",
    "\n",
    "            # this trick is just for model stability (do not touch it)\n",
    "            log_std = torch.tanh(log_std)\n",
    "            mu, log_std = mu.squeeze(-1), log_std.squeeze(-1)\n",
    "\n",
    "            # ====\n",
    "            # your code\n",
    "            # scale z to sigma and shift to mu get epsilon (reparametrization trick)\n",
    "            eps = \n",
    "            # ====\n",
    "        else:\n",
    "            # if we do not use autoregressive prior prior_log_prob is log p(z) = log p(epsilob)\n",
    "            eps = z\n",
    "            log_std = 0.0\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # compute log p(z) = log p(epsilon) + log |d epsilon / d z|\n",
    "        # 1) compute prior log prob (logarithm of standart normal distribution): log p(epsilon)\n",
    "        # 2) add logarithm of determinant (in out case it is just log_std): log |d epsilon / d z|\n",
    "        prior_log_prob = \n",
    "        prior_log_prob += \n",
    "        # ====\n",
    "\n",
    "        # kl loss is difference between encoder log prob and prior log prob\n",
    "        kl_loss = (enc_log_prob - prior_log_prob).sum(1).mean()\n",
    "        return {\n",
    "            'total': recon_loss + kl_loss,\n",
    "            'recon_loss': recon_loss,\n",
    "            'kl_loss': kl_loss\n",
    "        }\n",
    "\n",
    "    def sample_prior(self, n):\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n, self.latent_size).cuda()\n",
    "            if self.use_afp:\n",
    "                for i in range(self.latent_size):\n",
    "                    mu, log_std = self.made(z)[:, i].chunk(2, dim=-1)\n",
    "                    log_std = torch.tanh(log_std)\n",
    "                    mu, log_std = mu.squeeze(-1), log_std.squeeze(-1)\n",
    "                    # note! it is reverse transform and it is sequential\n",
    "                    z[:, i] = (z[:, i] - mu) * torch.exp(-log_std)\n",
    "        return z\n",
    "\n",
    "    def sample(self, n):\n",
    "        z = self.sample_prior(n)\n",
    "        with torch.no_grad():\n",
    "            out = self.decoder(z).cpu().permute(0, 2, 3, 1).numpy() * 0.5 + 0.5\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-6HwzClSRfj"
   },
   "source": [
    "Firstly, we fit standard VAE model without autorregressive prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JcMArVCA9ckD",
    "outputId": "f4250b96-e3fb-4d38-8445-6eb6bced55d2"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "# ====\n",
    "\n",
    "train_data, test_data = load_pickle(os.path.join('drive', 'My Drive', 'DGM', 'homework_supplementary', 'cifar10.pkl'))\n",
    "\n",
    "train_data = (np.transpose(train_data, (0, 3, 1, 2)) / 255.).astype('float32')\n",
    "test_data = (np.transpose(test_data, (0, 3, 1, 2)) / 255.).astype('float32')\n",
    "\n",
    "model = AFVAE((3, 32, 32), 16, use_afp=False).cuda()\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "train_losses, test_losses = train_model(model, train_loader, test_loader, epochs=EPOCHS, lr=LR)\n",
    "samples = model.sample(100) * 255\n",
    "\n",
    "x = next(iter(test_loader))[:50].cuda()\n",
    "with torch.no_grad():\n",
    "    z = model.encoder(2 * x - 1)[0]\n",
    "    x_recon = model.decoder(z).cpu().permute(0, 2, 3, 1).numpy() * 0.5 + 0.5\n",
    "x = x.cpu().permute(0, 2, 3, 1).numpy()\n",
    "reconstructions = np.stack((x, x_recon), axis=1).reshape((-1, 32, 32, 3)) * 255\n",
    "\n",
    "x = next(iter(test_loader))[:20].cuda()\n",
    "with torch.no_grad():\n",
    "    x = 2 * x - 1\n",
    "    z, _ = model.encoder(x)\n",
    "    z1, z2 = z.chunk(2, dim=0)\n",
    "    interps = [model.decoder(z1 * (1 - alpha) + z2 * alpha) for alpha in np.linspace(0, 1, 10)]\n",
    "    interps = torch.stack(interps, dim=1).view(-1, 3, 32, 32)\n",
    "    interps = torch.clamp(interps, -1, 1) * 0.5 + 0.5\n",
    "interps = interps.permute(0, 2, 3, 1).cpu().numpy() * 255\n",
    "\n",
    "samples, reconstructions, interps = np.clip(samples, 0, 255), np.clip(reconstructions, 0, 255), np.clip(interps, 0, 255)\n",
    "\n",
    "samples, reconstructions, interps = samples.astype('float32'), reconstructions.astype('float32'), interps.astype('float32')\n",
    "for key, value in test_losses.items():\n",
    "    print('{}: {:.4f}'.format(key, value[-1]))\n",
    "plot_training_curves(train_losses, test_losses)\n",
    "show_samples(samples, title='Samples')\n",
    "show_samples(reconstructions, title='Reconstructions')\n",
    "show_samples(interps, title='Interpolations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "KDK-XPiAOS-3",
    "outputId": "e872eac9-bb5a-4df7-c7d5-0526b80127eb"
   },
   "outputs": [],
   "source": [
    "z_prior = model.sample_prior(5000).cpu().detach().numpy()\n",
    "show_2d_latents(z_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTk3Qi7ESDXL"
   },
   "source": [
    "In this case the latent space is just standard normal.\n",
    "\n",
    "Now we fit the true AFVAE model with MADE in prior space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hkr88IxbIN3h",
    "outputId": "86967cf5-d5cf-417f-835b-7b8d0620640e"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "# ====\n",
    "\n",
    "train_data, test_data = load_pickle(os.path.join('drive', 'My Drive', 'DGM', 'homework_supplementary', 'cifar10.pkl'))\n",
    "\n",
    "train_data = (np.transpose(train_data, (0, 3, 1, 2)) / 255.).astype('float32')\n",
    "test_data = (np.transpose(test_data, (0, 3, 1, 2)) / 255.).astype('float32')\n",
    "\n",
    "model = AFVAE((3, 32, 32), 16, use_afp=True).cuda()\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "train_losses, test_losses = train_model(model, train_loader, test_loader, epochs=EPOCHS, lr=LR)\n",
    "samples = model.sample(100) * 255\n",
    "\n",
    "x = next(iter(test_loader))[:50].cuda()\n",
    "with torch.no_grad():\n",
    "    z = model.encoder(2 * x - 1)[0]\n",
    "    x_recon = model.decoder(z).cpu().permute(0, 2, 3, 1).numpy() * 0.5 + 0.5\n",
    "x = x.cpu().permute(0, 2, 3, 1).numpy()\n",
    "reconstructions = np.stack((x, x_recon), axis=1).reshape((-1, 32, 32, 3)) * 255\n",
    "\n",
    "x = next(iter(test_loader))[:20].cuda()\n",
    "with torch.no_grad():\n",
    "    x = 2 * x - 1\n",
    "    z, _ = model.encoder(x)\n",
    "    z1, z2 = z.chunk(2, dim=0)\n",
    "    interps = [model.decoder(z1 * (1 - alpha) + z2 * alpha) for alpha in np.linspace(0, 1, 10)]\n",
    "    interps = torch.stack(interps, dim=1).view(-1, 3, 32, 32)\n",
    "    interps = torch.clamp(interps, -1, 1) * 0.5 + 0.5\n",
    "interps = interps.permute(0, 2, 3, 1).cpu().numpy() * 255\n",
    "\n",
    "samples, reconstructions, interps = np.clip(samples, 0, 255), np.clip(reconstructions, 0, 255), np.clip(interps, 0, 255)\n",
    "\n",
    "samples, reconstructions, interps = samples.astype('float32'), reconstructions.astype('float32'), interps.astype('float32')\n",
    "for key, value in test_losses.items():\n",
    "    print('{}: {:.4f}'.format(key, value[-1]))\n",
    "plot_training_curves(train_losses, test_losses)\n",
    "show_samples(samples, title='Samples')\n",
    "show_samples(reconstructions, title='Reconstructions')\n",
    "show_samples(interps, title='Interpolations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "fGVekfe-nzR5",
    "outputId": "9522aacd-060b-4a3e-b1ae-5fe9644b96cc"
   },
   "outputs": [],
   "source": [
    "model.made.visualize_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "tV0IYsjjN2FH",
    "outputId": "10960e3c-801c-4539-f9b9-6385db69e78c"
   },
   "outputs": [],
   "source": [
    "z_prior = model.sample_prior(5000).cpu().detach().numpy()\n",
    "# ====\n",
    "# your code\n",
    "# try to find two latent units that will give the distribution that is not standard normal\n",
    "idx1 = \n",
    "idx2 = \n",
    "# ====\n",
    "show_2d_latents(z_prior[..., [idx1, idx2]])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
