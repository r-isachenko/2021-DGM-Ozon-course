{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opening-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "outdoor-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": [\"Helvetica\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-filing",
   "metadata": {},
   "source": [
    "# Forward KL vs Reverse KL vs JSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "recent-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaussian_log_prob_fn(mu, sigma):\n",
    "    d = torch.distributions.normal.Normal(mu, sigma)\n",
    "    fn = lambda x: d.log_prob(x)\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quiet-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(np.linspace(-6,6,500)).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "boxed-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_gaussian = get_gaussian_log_prob_fn(mu=0, sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "statistical-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X.numpy(), single_gaussian(X).exp().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "frequent-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaussian_mixture_log_prob_fn():\n",
    "    # define a mixture of gaussians\n",
    "    k = 0.8\n",
    "    a = get_gaussian_log_prob_fn(-2, 0.5)\n",
    "    b = get_gaussian_log_prob_fn(2, 0.5)\n",
    "    \n",
    "    # your code here\n",
    "    fn = None\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "southern-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions(X, real_log_prob, model_log_prob=None, title=None, figsize=None, savepath=None):\n",
    "    with torch.no_grad():\n",
    "        res1 = real_log_prob(X).exp()\n",
    "        if model_log_prob is not None:\n",
    "            res2 = model_log_prob(X).exp()\n",
    "\n",
    "    if figsize is None:\n",
    "        figsize = (5, 5)\n",
    "    plt.figure(figsize=figsize)    \n",
    "    plt.plot(X.numpy(), res1.numpy(), color='b', linewidth=3, label=r'$\\pi(\\mathbf{x})$')\n",
    "    plt.fill_between(X.numpy(), np.zeros_like(X.numpy()), res1.numpy(),  color='b', alpha=0.2)\n",
    "    if model_log_prob is not None:\n",
    "        plt.plot(X.numpy(), res2.numpy(), 'g', linewidth=3, label=r'$p(\\mathbf{x} | \\mu, \\sigma)$')\n",
    "        plt.fill_between(X.numpy(), np.zeros_like(X.numpy()), res2.numpy(),  color='g', alpha=0.2)\n",
    "    if title is not None:\n",
    "        plt.title(title, fontsize=20)\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    if savepath is not None:\n",
    "        plt.savefig(savepath)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "photographic-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture_gaussian = get_gaussian_mixture_log_prob_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "closing-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions(X, single_gaussian, title='single gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "perfect-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions(X, mixture_gaussian, title='mixture of gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "weighted-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions(X, mixture_gaussian, single_gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-graph",
   "metadata": {},
   "source": [
    "<img src=\"pics/kld_jsd.jpg\" width=800 height=800 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "amended-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLD(X, log_prob1, log_prob2):\n",
    "    # p and q return log pdf\n",
    "    # X - some linspace\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "optimum-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSD(X, log_prob1, log_prob2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "successful-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_KL_loss_fn = lambda X, real_log_prob, model_log_prob: KLD(X, real_log_prob, model_log_prob)\n",
    "reverse_KL_loss_fn = lambda X, real_log_prob, model_log_prob: KLD(X, model_log_prob, real_log_prob)\n",
    "JSD_loss_fn = lambda X, real_log_prob, model_log_prob: JSD(X, real_log_prob, model_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "passive-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_start = -1\n",
    "sigma_start = 0.5\n",
    "\n",
    "# define trainable params\n",
    "mu = torch.tensor(mu_start, requires_grad=True, dtype=torch.double)\n",
    "sigma = torch.tensor(sigma_start, requires_grad=True, dtype=torch.double)\n",
    "single_gaussian = get_gaussian_log_prob_fn(mu, sigma)\n",
    "\n",
    "plot_distributions(X, mixture_gaussian, single_gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "conscious-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_task(optimizer, loss_fn, real_log_prob, model_log_prob, n_iters):\n",
    "    for it in range(n_iters):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss_fn(X, real_log_prob, model_log_prob)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if it % 5 == 0: \n",
    "            plot_distributions(X, mixture_gaussian, single_gaussian, title=f'step: {it}, loss: {loss:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-production",
   "metadata": {},
   "source": [
    "### Forward KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "persistent-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITERS = 50\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "mu_start = -1\n",
    "sigma_start = 0.5\n",
    "\n",
    "mu = torch.tensor(mu_start, requires_grad=True, dtype=torch.double)\n",
    "sigma = torch.tensor(sigma_start, requires_grad=True, dtype=torch.double)\n",
    "single_gaussian = get_gaussian_log_prob_fn(mu, sigma)\n",
    "\n",
    "optimizer = torch.optim.SGD([mu, sigma], lr=LEARNING_RATE)\n",
    "\n",
    "solve_task(optimizer, forward_KL_loss_fn, mixture_gaussian, single_gaussian, N_ITERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "incredible-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions(\n",
    "    X, \n",
    "    mixture_gaussian, \n",
    "    single_gaussian, \n",
    "    title='forward KL', \n",
    "    #savepath='forward_KL.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-pierre",
   "metadata": {},
   "source": [
    "## Reverse KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions(\n",
    "    X, \n",
    "    mixture_gaussian, \n",
    "    single_gaussian, \n",
    "    title='reverse KL', \n",
    "    #savepath='reverse_KL.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-dover",
   "metadata": {},
   "source": [
    "## Jensen Shannon optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions(\n",
    "    X, \n",
    "    mixture_gaussian, \n",
    "    single_gaussian, \n",
    "    title='JSD', \n",
    "    #savepath='JSD.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-alarm",
   "metadata": {},
   "source": [
    "# Vanilla GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-national",
   "metadata": {},
   "source": [
    "<img src=\"pics/gan_objective.jpg\" width=800 height=800 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "tracked-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_model(hiddens):\n",
    "    assert len(hiddens) > 1\n",
    "\n",
    "    modules = []\n",
    "    for in_, out_ in zip(hiddens[:-2], hiddens[1:-1]):\n",
    "        modules.extend([nn.Linear(in_, out_), nn.ReLU()])\n",
    "\n",
    "    modules.append(nn.Linear(hiddens[-2], hiddens[-1]))\n",
    "\n",
    "    return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "solved-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define shallow nn\n",
    "gen_hiddens = [1,64,64,1]\n",
    "dis_hiddens = [1,64,64,1]\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "higher-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 2\n",
    "noise_fn = lambda x: torch.rand((x, 1), device='cpu')-2\n",
    "data_fn = lambda x: mu+torch.randn((x, 1), device='cpu')\n",
    "data_pdf = lambda X: norm.pdf(X-mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "arranged-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gan_data(data_fn, noise_fn, data_pdf=None):\n",
    "    noise = noise_fn(5000).numpy().flatten()\n",
    "    target = data_fn(5000).numpy().flatten()\n",
    "    \n",
    "    plt.hist(noise, label='noise', alpha=0.5, density=True, color='b')\n",
    "    plt.hist(target, label='target', alpha=0.5, density=True, color='g')\n",
    "    if data_pdf is not None:\n",
    "        x = np.linspace(-6,6,100)\n",
    "        plt.plot(x, data_pdf(x), 'g', label='real distibution')\n",
    "\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "phantom-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gan_data(data_fn, noise_fn, data_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "critical-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaGAN():\n",
    "    def __init__(self, G, D, noise_fn, data_fn,\n",
    "                 batch_size=32, device='cpu', lr_D=1e-3, lr_G=2e-4):\n",
    "        \"\"\"A GAN class for holding and training a generator and discriminator\n",
    "        Args:\n",
    "            G: a Ganerator network\n",
    "            D: A Discriminator network\n",
    "            noise_fn: function f(num: int) -> pytorch tensor, (latent vectors)\n",
    "            data_fn: function f(num: int) -> pytorch tensor, (real samples)\n",
    "            batch_size: training batch size\n",
    "            device: cpu or CUDA\n",
    "            lr_D: learning rate for the discriminator\n",
    "            lr_G: learning rate for the generator\n",
    "        \"\"\"\n",
    "        self.G = G\n",
    "        self.G = self.G.to(device)\n",
    "        self.D = D\n",
    "        self.D = self.D.to(device)\n",
    "        self.noise_fn = noise_fn\n",
    "        self.data_fn = data_fn\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        # !\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optim_D = optim.Adam(D.parameters(),\n",
    "                                  lr=lr_D, betas=(0.5, 0.999))\n",
    "        self.optim_G = optim.Adam(G.parameters(),\n",
    "                                  lr=lr_G, betas=(0.5, 0.999))\n",
    "        # is needed in D train loop\n",
    "        self.target_ones = torch.ones((batch_size, 1)).to(device)\n",
    "        self.target_zeros = torch.zeros((batch_size, 1)).to(device)\n",
    "    \n",
    "    def generate_samples(self, latent_vec=None, num=None):\n",
    "        \"\"\"Sample from the generator.\n",
    "        Args:\n",
    "            latent_vec: A pytorch latent vector or None\n",
    "            num: The number of samples to generate if latent_vec is None\n",
    "        If latent_vec and num are None then us self.batch_size random latent\n",
    "        vectors.\n",
    "        ! We don't need grad for generated samples\n",
    "        \"\"\"\n",
    "        num = self.batch_size if num is None else num\n",
    "        latent_vec = self.noise_fn(num) if latent_vec is None else latent_vec\n",
    "        # your code here\n",
    "        samples = []\n",
    "        \n",
    "        return samples\n",
    "\n",
    "    def train_step_G(self):\n",
    "        \"\"\"Train the generator one step and return the loss.\"\"\"\n",
    "        self.G.zero_grad()\n",
    "        latent_vec = self.noise_fn(self.batch_size)\n",
    "        # your code here\n",
    "        # use self.target_ones\n",
    "        loss = 0\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    def train_step_D(self):\n",
    "        \"\"\"Train the discriminator one step and return the losses.\"\"\"\n",
    "        self.D.zero_grad()\n",
    "\n",
    "        # real samples\n",
    "        real_samples = self.data_fn(self.batch_size)\n",
    "        # calc real loss\n",
    "        # you code here\n",
    "        loss_real = 0\n",
    "\n",
    "        # generated samples\n",
    "        latent_vec = self.noise_fn(self.batch_size)\n",
    "        # calc fake loss\n",
    "        # you shouldn't optimize G here\n",
    "        # you code here\n",
    "        \n",
    "        loss_fake = 0\n",
    "\n",
    "        # combine\n",
    "        loss = (loss_real + loss_fake) / 2\n",
    "        loss.backward()\n",
    "        self.optim_D.step()\n",
    "        return loss_real.item(), loss_fake.item()\n",
    "\n",
    "    def train_step(self):\n",
    "        \"\"\"Train both networks and return the losses.\"\"\"\n",
    "        loss_D = self.train_step_D()\n",
    "        loss_G = self.train_step_G()\n",
    "        return loss_G, loss_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fleet-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_hiddens = [1,64,64,1]\n",
    "dis_hiddens = [1,64,64,1]\n",
    "G = get_simple_model(gen_hiddens)\n",
    "D = nn.Sequential(*get_simple_model(dis_hiddens), nn.Sigmoid())\n",
    "\n",
    "gan = VanillaGAN(G, D, noise_fn, data_fn, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "intermediate-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_GAN(gan, data_pdf=None):\n",
    "    size = 500\n",
    "    x = np.linspace(-6,6,100)\n",
    "    bins = np.linspace(-6,6,60)\n",
    "    real_data = gan.data_fn(size)\n",
    "    noise = gan.noise_fn(size)\n",
    "    sampled_data = gan.generate_samples(noise)\n",
    "    \n",
    "    plt.hist(noise.numpy(), label='noise', alpha=0.5, density=True, color='b', bins=bins)\n",
    "    plt.hist(real_data.numpy(), label='real data', alpha=0.5, density=True, color='g', bins=bins)\n",
    "    plt.hist(sampled_data.numpy(), label='G samples', alpha=0.5, density=True, color='r', bins=bins)\n",
    "    \n",
    "    if data_pdf is not None:\n",
    "        plt.plot(x, data_pdf(x), 'g', label='real distibution')\n",
    "    with torch.no_grad():\n",
    "        plt.plot(x, gan.D(torch.from_numpy(x).float().unsqueeze(-1)).numpy(), 'b', label='D distibution')\n",
    "    \n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "silver-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_GAN(gan, data_pdf=data_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "skilled-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 2\n",
    "noise_fn = lambda x: torch.rand((x, 1), device='cpu')-2\n",
    "data_fn = lambda x: mu+torch.randn((x, 1), device='cpu')\n",
    "data_pdf = lambda X: norm.pdf(X-mu)\n",
    "\n",
    "gen_hiddens = [1,64,64,1]\n",
    "dis_hiddens = [1,64,64,1]\n",
    "G = get_simple_model(gen_hiddens)\n",
    "D = nn.Sequential(*get_simple_model(dis_hiddens), nn.Sigmoid())\n",
    "\n",
    "gan = VanillaGAN(G, D, noise_fn, data_fn, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "coordinated-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gan_data(data_fn, noise_fn, data_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "modular-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_GAN(gan, data_pdf=data_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fossil-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batches = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "knowing-dispute",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "step_size = 1\n",
    "loss_g, loss_d_real, loss_d_fake = [], [], []\n",
    "start = time()\n",
    "for epoch in range(epochs):\n",
    "    break\n",
    "    loss_g_running, loss_d_real_running, loss_d_fake_running = 0, 0, 0\n",
    "    for i,batch in enumerate(range(batches)):\n",
    "        #lg_, (ldr_, ldf_) = gan.train_step()\n",
    "        ldr_, ldf_ = gan.train_step_D()\n",
    "        if i%step_size == 0:\n",
    "            print(i)\n",
    "            print('D train step')\n",
    "            visualize_GAN(gan)\n",
    "        lg_ = gan.train_step_G()\n",
    "        if i%step_size == 0:\n",
    "            print('G train step')\n",
    "            visualize_GAN(gan)\n",
    "        \n",
    "        loss_g_running += lg_\n",
    "        loss_d_real_running += ldr_\n",
    "        loss_d_fake_running += ldf_\n",
    "    loss_g.append(loss_g_running / batches)\n",
    "    loss_d_real.append(loss_d_real_running / batches)\n",
    "    loss_d_fake.append(loss_d_fake_running / batches)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} ({int(time() - start)}s):\"\n",
    "          f\" G={loss_g[-1]:.3f},\"\n",
    "          f\" Dr={loss_d_real[-1]:.3f},\"\n",
    "          f\" Df={loss_d_fake[-1]:.3f}\")\n",
    "    visualize_GAN(gan, data_pdf=data_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "medium-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = 3\n",
    "mu2 = -3\n",
    "k = 0.7\n",
    "\n",
    "noise_fn = lambda x: torch.rand((x, 1), device='cpu') - 0.5\n",
    "\n",
    "def data_fn(x):\n",
    "    a = mu1 + torch.randn((x, 1), device='cpu')\n",
    "    b = mu2 + torch.randn((x, 1), device='cpu')\n",
    "    mask = np.random.rand(x) < k\n",
    "    mask = mask[:, None]\n",
    "    samples = (a * mask + b * (1 - mask))\n",
    "    \n",
    "    return samples.float()\n",
    "\n",
    "data_pdf = lambda X: k*norm.pdf(X-mu1)+(1-k)*norm.pdf(X-mu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "young-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gan_data(data_fn, noise_fn, data_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "published-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_hiddens = [1,64,64,1]\n",
    "dis_hiddens = [1,64,64,1]\n",
    "G = get_simple_model(gen_hiddens)\n",
    "D = nn.Sequential(*get_simple_model(dis_hiddens), nn.Sigmoid())\n",
    "\n",
    "gan = VanillaGAN(G, D, noise_fn, data_fn, device='cpu')\n",
    "loss_g, loss_d_real, loss_d_fake = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "intelligent-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batches = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "associate-glasgow",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "for epoch in range(epochs):\n",
    "    #break\n",
    "    loss_g_running, loss_d_real_running, loss_d_fake_running = 0, 0, 0\n",
    "    for i,batch in enumerate(range(batches)):\n",
    "        lg_, (ldr_, ldf_) = gan.train_step()\n",
    "        loss_g_running += lg_\n",
    "        loss_d_real_running += ldr_\n",
    "        loss_d_fake_running += ldf_\n",
    "    loss_g.append(loss_g_running / batches)\n",
    "    loss_d_real.append(loss_d_real_running / batches)\n",
    "    loss_d_fake.append(loss_d_fake_running / batches)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} ({int(time() - start)}s):\"\n",
    "          f\" G={loss_g[-1]:.3f},\"\n",
    "          f\" Dr={loss_d_real[-1]:.3f},\"\n",
    "          f\" Df={loss_d_fake[-1]:.3f}\")\n",
    "    visualize_GAN(gan, data_pdf=data_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-disability",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
